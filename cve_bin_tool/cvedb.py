"""
Retrieval access and caching of NIST CVE database
"""
import asyncio
import datetime
import glob
import gzip
import hashlib
import json
import logging
import os
import re
import shutil
import sqlite3
import sys
import traceback
import urllib.request as request

import aiohttp
from bs4 import BeautifulSoup
from rich.progress import Progress, track

from .async_utils import FileIO, GzipFile
from .version import check_latest_version
from .log import LOGGER

logging.basicConfig(level=logging.DEBUG)

# database defaults
DISK_LOCATION_DEFAULT = os.path.join(os.path.expanduser("~"), ".cache", "cve-bin-tool")
DBNAME = "cve.db"
OLD_CACHE_DIR = os.path.join(os.path.expanduser("~"), ".cache", "cvedb")


class EmptyCache(Exception):
    """
    Raised when NVD is opened when verify=False and there are no files in the
    cache.
    """


class CVEDataForYearNotInCache(Exception):
    """
    Raised when the CVE data for a year is not present in the cache.
    """


class AttemptedToWriteOutsideCachedir(Exception):
    """
    Raised if we attempted to write to a file that would have been outside the
    cachedir.
    """


class SHAMismatch(Exception):
    """
    Raised if the sha of a file in the cache was not what it should be.
    """


def log_traceback(func, *args, **kwargs):
    """
    Multiprocessing won't print tracebacks, so log them
    """
    logger = logging.getLogger(__name__ + "." + func.__name__)
    try:
        return func(*args, logger=logger, **kwargs)
    except:
        logger.error(traceback.format_exc().strip())
        raise


class CVEDB:
    """
    Downloads NVD data in json form and stores it on disk in a cache.
    """

    CACHEDIR = DISK_LOCATION_DEFAULT
    FEED = "https://nvd.nist.gov/vuln/data-feeds"
    LOGGER = LOGGER.getChild("CVEDB")
    NVDCVE_FILENAME_TEMPLATE = "nvdcve-1.1-{}.json.gz"
    META_REGEX = re.compile(r"https://.*/json/.*-[0-9]*\.[0-9]*-[0-9]*\.meta")
    RANGE_UNSET = ""

    def __init__(
        self, verify=True, feed=None, cachedir=None, version_check=True, session=None
    ):
        self.verify = verify
        self.feed = feed if feed is not None else self.FEED
        self.cachedir = cachedir if cachedir is not None else self.CACHEDIR
        # Will be true if refresh was successful
        self.was_updated = False

        # version update
        self.version_check = version_check

        # set up the db if needed
        self.disk_location = DISK_LOCATION_DEFAULT
        self.dbname = os.path.join(self.disk_location, DBNAME)
        self.connection = None
        self.session = session

    async def getmeta(self, meta_url):
        async with self.session.get(meta_url) as response:
            return (
                meta_url.replace(".meta", ".json.gz"),
                dict(
                    [
                        line.split(":", maxsplit=1)
                        for line in (await response.text()).splitlines()
                        if ":" in line
                    ]
                ),
            )

    async def nist_scrape(self):
        async with self.session.get(self.feed) as response:
            page = await response.text()
            json_meta_links = self.META_REGEX.findall(page)
            return dict(
                await asyncio.gather(
                    *[self.getmeta(meta_url) for meta_url in json_meta_links]
                )
            )

    async def cache_update(self, url, sha, chunk_size=16 * 1024):
        """
        Update the cache for a single year of NVD data.
        """
        filename = url.split("/")[-1]
        # Ensure we only write to files within the cachedir
        filepath = os.path.abspath(os.path.join(self.cachedir, filename))
        if not filepath.startswith(os.path.abspath(self.cachedir)):
            raise AttemptedToWriteOutsideCachedir(filepath)
        # Validate the contents of the cached file
        if os.path.isfile(filepath):
            # Validate the sha and write out
            sha = sha.upper()
            calculate = hashlib.sha256()
            async with GzipFile(filepath, "rb") as f:
                chunk = await f.read(chunk_size)
                while chunk:
                    calculate.update(chunk)
                    chunk = await f.read(chunk_size)
            # Validate the sha and exit if it is correct, otherwise update
            gotsha = calculate.hexdigest().upper()
            if gotsha != sha:
                os.unlink(filepath)
                self.LOGGER.warning(
                    f"SHA mismatch for {filename} (have: {gotsha}, want: {sha})"
                )
            else:
                self.LOGGER.debug(f"Correct SHA for {filename}")
                return
        self.LOGGER.info(f"Updating CVE cache for {filename}")

        async with self.session.get(url) as response:
            gzip_data = await response.read()
            json_data = gzip.decompress(gzip_data)
            gotsha = hashlib.sha256(json_data).hexdigest().upper()
            async with FileIO(filepath, "wb") as filepath_handle:
                await filepath_handle.write(gzip_data)
        # Raise error if there was an issue with the sha
        if gotsha != sha:
            # Remove the file if there was an issue
            # exit(100)
            os.unlink(filepath)
            raise SHAMismatch(f"{url} (have: {gotsha}, want: {sha})")

    async def refresh(self):
        """Refresh the cve database and check for new version.
        """
        # refresh the database
        if not os.path.isdir(self.cachedir):
            os.makedirs(self.cachedir)
        if not self.session:
            self.session = aiohttp.ClientSession()
        metadata = await self.nist_scrape()
        await asyncio.gather(
            *[
                self.cache_update(url, meta["sha256"])
                for url, meta in metadata.items()
                if meta is not None
            ]
        )
        self.was_updated = True

        # check for the latest version
        if self.version_check:
            self.LOGGER.info("Checking if there is a newer version.")
            check_latest_version()
        await self.session.close()
        self.session = None

    def refresh_cache_and_update_db(self):
        self.LOGGER.info("Updating CVE data. This will take a few minutes.")
        # refresh the nvd cache
        loop = asyncio.get_event_loop()
        if sys.platform.startswith("win"):
            if isinstance(loop, asyncio.SelectorEventLoop):
                loop = asyncio.ProactorEventLoop()
                asyncio.set_event_loop(loop)
        aws = asyncio.ensure_future(self.refresh())
        loop.run_until_complete(aws)

        # if the database isn't open, open it
        self.init_database()
        self.populate_db()

    def get_cvelist_if_stale(self):
        """ Update if the local db is more than one day old.
        This avoids the full slow update with every execution.
        """
        if not os.path.isfile(self.dbname) or (
            datetime.datetime.today()
            - datetime.datetime.fromtimestamp(os.path.getmtime(self.dbname))
        ) > datetime.timedelta(hours=24):
            self.refresh_cache_and_update_db()
        else:
            self.LOGGER.info(
                "Using cached CVE data (<24h old). Use -u now to update immediately."
            )

    def init_database(self):
        """ Initialize db tables used for storing cve/version data """
        self.open()
        cursor = self.connection.cursor()
        cve_data_create = """
        CREATE TABLE IF NOT EXISTS cve_severity (
            cve_number TEXT,
            severity TEXT,
            description TEXT,
            score INTEGER,
            cvss_version INTEGER,
            PRIMARY KEY(cve_number)
        )
        """
        version_range_create = """
        CREATE TABLE IF NOT EXISTS cve_range (
            cve_number TEXT,
            vendor TEXT,
            product TEXT,
            version TEXT,
            versionStartIncluding TEXT,
            versionStartExcluding TEXT,
            versionEndIncluding TEXT,
            versionEndExcluding TEXT
        )
        """
        index_range = "CREATE INDEX IF NOT EXISTS product_index ON cve_range (cve_number, vendor, product)"
        cursor.execute(cve_data_create)
        cursor.execute(version_range_create)
        cursor.execute(index_range)
        self.connection.commit()

    def populate_db(self):
        """ Function that populates the database from the JSON.

        WARNING: After some inspection of the data, we are assuming that start/end ranges are kept together
        in single nodes.  This isn't *required* by the json so may not be true everywhere.  If that's the case,
        we'll need a better parser to match those together.
        """
        self.open()
        cursor = self.connection.cursor()

        insert_severity = """
        INSERT or REPLACE INTO cve_severity(
            CVE_number,
            severity, 
            description, 
            score, 
            cvss_version
        )
        VALUES (?, ?, ?, ?, ?)
        """
        insert_cve_range = """
        INSERT or REPLACE INTO cve_range(
            cve_number,
            vendor,
            product,
            version,
            versionStartIncluding,
            versionStartExcluding,
            versionEndIncluding,
            versionEndExcluding
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """
        del_cve_range = "DELETE from cve_range where CVE_number=?"

        for year in track(self.years(), description="Updating CVEs from NVD... "):
            cve_data = self.year(year)
            self.LOGGER.debug(
                f'Time = {datetime.datetime.today().strftime("%H:%M:%S")}'
            )
            for cve_item in cve_data["CVE_Items"]:
                # the information we want:
                # CVE ID, Severity, Score ->
                # affected {Vendor(s), Product(s), Version(s)}
                cve = {
                    "ID": cve_item["cve"]["CVE_data_meta"]["ID"],
                    "description": cve_item["cve"]["description"]["description_data"][
                        0
                    ]["value"],
                    "severity": "unknown",
                    "score": "unknown",
                    "CVSS_version": "unknown",
                }
                # Get CVSSv3 or CVSSv2 score for output.
                # Details are left as an exercise to the user.
                if "baseMetricV3" in cve_item["impact"]:
                    cve["severity"] = cve_item["impact"]["baseMetricV3"]["cvssV3"][
                        "baseSeverity"
                    ]
                    cve["score"] = cve_item["impact"]["baseMetricV3"]["cvssV3"][
                        "baseScore"
                    ]
                    cve["CVSS_version"] = 3
                elif "baseMetricV2" in cve_item["impact"]:
                    cve["severity"] = cve_item["impact"]["baseMetricV2"]["severity"]
                    cve["score"] = cve_item["impact"]["baseMetricV2"]["cvssV2"][
                        "baseScore"
                    ]
                    cve["CVSS_version"] = 2

                # self.LOGGER.debug(
                #    "Severity: {} ({}) v{}".format(
                #        CVE["severity"], CVE["score"], CVE["CVSS_version"]
                #    )
                # )

                cursor.execute(
                    insert_severity,
                    [
                        cve["ID"],
                        cve["severity"],
                        cve["description"],
                        cve["score"],
                        cve["CVSS_version"],
                    ],
                )

                # Delete any old range entries for this CVE_number
                cursor.execute(del_cve_range, (cve["ID"],))

                # walk the nodes with version data
                # return list of versions
                affects_list = []
                if "configurations" in cve_item:
                    for node in cve_item["configurations"]["nodes"]:
                        # self.LOGGER.debug("NODE: {}".format(node))
                        affects_list.extend(self.parse_node(node))
                        if "children" in node:
                            for child in node["children"]:
                                affects_list.extend(self.parse_node(child))
                # self.LOGGER.debug("Affects: {}".format(affects_list))
                cursor.executemany(
                    insert_cve_range,
                    [
                        (
                            cve["ID"],
                            affected["vendor"],
                            affected["product"],
                            affected["version"],
                            affected["versionStartIncluding"],
                            affected["versionStartExcluding"],
                            affected["versionEndIncluding"],
                            affected["versionEndExcluding"],
                        )
                        for affected in affects_list
                    ],
                )
            self.connection.commit()

        # supplemental data gets added here
        self.supplement_curl()

    def parse_node(self, node):
        affects_list = []
        if "cpe_match" in node:
            for cpe_match in node["cpe_match"]:
                # self.LOGGER.debug(cpe_match["cpe23Uri"])
                cpe_split = cpe_match["cpe23Uri"].split(":")
                affects = {
                    "vendor": cpe_split[3],
                    "product": cpe_split[4],
                    "version": cpe_split[5],
                }

                # self.LOGGER.debug(
                #    "Vendor: {} Product: {} Version: {}".format(
                #        affects["vendor"], affects["product"], affects["version"]
                #    )
                # )
                # if we have a range (e.g. version is *) fill it out, and put blanks where needed
                range_fields = [
                    "versionStartIncluding",
                    "versionStartExcluding",
                    "versionEndIncluding",
                    "versionEndExcluding",
                ]
                for field in range_fields:
                    if field in cpe_match:
                        affects[field] = cpe_match[field]
                    else:
                        affects[field] = self.RANGE_UNSET

                affects_list.append(affects)
        return affects_list

    @staticmethod
    def curl_versions():
        regex = re.compile(r"vuln-(\d+.\d+.\d+)\.html")
        with request.urlopen(
            "https://curl.haxx.se/docs/vulnerabilities.html"
        ) as response:
            html = response.read().decode()
        matches = regex.finditer(html)
        return [match.group(1) for match in matches]

    def supplement_curl(self):
        """
        Get additional CVE data directly from the curl website amd add it to the cvedb
        """
        self.open()
        insert_cve_range = """
        INSERT or REPLACE INTO cve_range(
            cve_number,
            vendor,
            product,
            version,
            versionStartIncluding,
            versionStartExcluding,
            versionEndIncluding,
            versionEndExcluding
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """
        cursor = self.connection.cursor()

        for version in track(
            self.curl_versions(), description="Updating CVEs from curl..."
        ):
            with request.urlopen(
                f"https://curl.haxx.se/docs/vuln-{version}.html"
            ) as response:
                html = response.read().decode()
            soup = BeautifulSoup(html, "html.parser")
            table = soup.find("table")
            if not table:
                continue
            headers = table.find_all("th")
            headers = list(map(lambda x: x.text.strip().lower(), headers))
            self.LOGGER.debug(headers)
            rows = table.find_all("tr")
            for row in rows:
                cols = row.find_all("td")
                values = (ele.text.strip() for ele in cols)
                if values:
                    data = dict(zip(headers, values))
                    cursor.execute(
                        insert_cve_range,
                        (
                            data["cve"],
                            "haxx",
                            "curl",
                            version,
                            data["from version"],
                            "",
                            data["to and including"],
                            "",
                        ),
                    )
            self.connection.commit()

    def year(self, year):
        """
        Return the dict of CVE data for the given year.
        """
        filename = os.path.join(
            self.cachedir, self.NVDCVE_FILENAME_TEMPLATE.format(year)
        )
        # Check if file exists
        if not os.path.isfile(filename):
            raise CVEDataForYearNotInCache(year)
        # Open the file and load the JSON data, log the number of CVEs loaded
        with gzip.open(filename, "rb") as fileobj:
            cves_for_year = json.load(fileobj)
            self.LOGGER.debug(
                f'Year {year} has {len(cves_for_year["CVE_Items"])} CVEs in dataset'
            )
            return cves_for_year

    def years(self):
        """
        Return the years we have NVD data for.
        """
        return sorted(
            [
                int(filename.split(".")[-3].split("-")[-1])
                for filename in glob.glob(
                    os.path.join(self.cachedir, "nvdcve-1.1-*.json.gz")
                )
            ]
        )

    def clear_cached_data(self):
        if os.path.exists(self.cachedir):
            self.LOGGER.warning(f"Deleting cachedir {self.cachedir}")
            shutil.rmtree(self.cachedir)
        # Remove files associated with pre-1.0 development tree
        if os.path.exists(OLD_CACHE_DIR):
            self.LOGGER.warning(f"Deleting old cachedir {OLD_CACHE_DIR}")
            shutil.rmtree(OLD_CACHE_DIR)

    def open(self):
        """ Opens connection to sqlite database."""
        self.connection = sqlite3.connect(self.dbname, check_same_thread=False)

    def close(self):
        """ Closes connection to sqlite database."""
        self.connection.close()
        self.connection = None

    def __enter__(self):
        """ Opens connection to sqlite database."""
        self.open()
        if not self.verify:
            self.LOGGER.error("Not verifying CVE DB cache")
            if not self.years():
                raise EmptyCache(self.cachedir)
        self.LOGGER.debug(f"Years present: {self.years()}")
        return self

    def __exit__(self, exc_type, exc, exc_tb):
        """ Closes connection to sqlite database."""
        self.close()


def refresh():
    with CVEDB():
        pass


if __name__ == "__main__":
    LOGGER.debug("Experimenting...")
    cvedb = CVEDB(DISK_LOCATION_DEFAULT)
    # cvedb.refresh()
    # print(cvedb.years())
    # connection = cvedb.init_database()
    # cvedb.populate_db(connection)
    # cvedb.supplement_curl()
    LOGGER.setLevel(logging.INFO)
    LOGGER.info("Getting cves for curl 7.34.0")
    LOGGER.info(cvedb.get_cves("haxx", "curl", "7.34.0"))
